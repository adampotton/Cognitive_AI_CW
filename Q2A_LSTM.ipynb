{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvNEzxmG7veimX+d4kF97j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/Cognitive_AI_CW/blob/main/Q2A_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/neurogym/neurogym.git\n",
        "%cd neurogym/\n",
        "! pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3gn4uXo2PPtk",
        "outputId": "dd4253f8-770a-4f56-9a43-4861d34050fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neurogym'...\n",
            "remote: Enumerating objects: 11100, done.\u001b[K\n",
            "remote: Counting objects: 100% (1002/1002), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 11100 (delta 928), reused 896 (delta 896), pack-reused 10098 (from 1)\u001b[K\n",
            "Receiving objects: 100% (11100/11100), 8.17 MiB | 14.94 MiB/s, done.\n",
            "Resolving deltas: 100% (8333/8333), done.\n",
            "/content/neurogym\n",
            "Obtaining file:///content/neurogym\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (1.26.4)\n",
            "Requirement already satisfied: gym<0.25,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (0.23.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (3.8.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurogym==0.0.2) (1.16.0)\n",
            "Installing collected packages: neurogym\n",
            "  Running setup.py develop for neurogym\n",
            "Successfully installed neurogym-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RPIsY6LowX",
        "outputId": "56558e7e-f78e-41f3-90d4-4fb5df75d3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import neurogym as ngym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_lstm_layers = 2, **kwargs):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_lstm_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_output)\n",
        "        return out, lstm_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUHo7Q4O0mZ",
        "outputId": "7aea8e04-0542-436e-b982-bb40655ebd1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dt': 100,                      # Set your timestep parameter\n",
        "    'hidden_size': 32,             # Set the hidden size for your LSTM\n",
        "    'batch_size': 16,               # Batch size for training\n",
        "    'seq_len': 100,                 # Sequence length for input data\n",
        "    'envid': 'ReadySetGo-v0',       # The task name\n",
        "    'gain': 1,                      # Example: Custom gain\n",
        "    'prod_margin': 50,              # Example: Custom production margin\n",
        "}\n",
        "\n",
        "env_kwargs = {\n",
        "    'dt': config['dt'],\n",
        "    'gain': config['gain'],        # Add gain parameter\n",
        "    'prod_margin': config['prod_margin'],  # Add production margin parameter\n",
        "}\n",
        "config['env_kwargs'] = env_kwargs\n",
        "\n",
        "dataset = ngym.Dataset(config['envid'], env_kwargs=config['env_kwargs'], batch_size=config['batch_size'], seq_len=config['seq_len'])\n",
        "env = dataset.env\n",
        "\n",
        "inputs, target = dataset()\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
        "print('Target has shape (SeqLen, Batch) =', target.shape)\n",
        "\n",
        "iter_steps = 1000\n",
        "report_freq = 100\n",
        "\n",
        "# Create an instance of the Class RNNNet\n",
        "net = LSTMNet(input_size, config['hidden_size'], output_size, num_lstm_layers = 1)\n",
        "print(net)\n",
        "\n",
        "def train_model(net, dataset, iter_steps, report_freq):\n",
        "\n",
        "    # Use Adam optimizer\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    start_time = time.time()\n",
        "    # Loop over training batches\n",
        "    print('Training network...')\n",
        "    for i in range(iter_steps):\n",
        "        # Generate input and target, convert to pytorch tensor\n",
        "        inputs, labels = dataset()\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output, _ = net(inputs)\n",
        "        output = output.view(-1, output_size)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()    # Does the update\n",
        "\n",
        "        # Compute accuracy for the current batch\n",
        "        batch_acc = (torch.argmax(output, dim=1) == labels).sum().item() / labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_acc += batch_acc\n",
        "\n",
        "        # Report every 100 steps\n",
        "        if i % report_freq == report_freq - 1:\n",
        "            running_loss /= report_freq\n",
        "            running_acc /= report_freq  # average accuracy over the last 100 batches\n",
        "            print('Step {}, Loss {:0.4f}, Accuracy {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, running_acc, time.time() - start_time))\n",
        "            running_loss = 0\n",
        "            running_acc = 0  # Reset for the next 100 steps\n",
        "    return net\n",
        "\n",
        "net = train_model(net, dataset, iter_steps, report_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "milzVKfVSmQT",
        "outputId": "52379a55-1b5d-4ef6-c10b-8f7d0660e24c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has shape (SeqLen, Batch, Dim) = torch.Size([100, 16, 3])\n",
            "Target has shape (SeqLen, Batch) = (100, 16)\n",
            "LSTMNet(\n",
            "  (lstm): LSTM(3, 32)\n",
            "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n",
            "Training network...\n",
            "Step 100, Loss 0.1495, Accuracy 0.9706, Time 1.0s\n",
            "Step 200, Loss 0.0975, Accuracy 0.9707, Time 2.0s\n",
            "Step 300, Loss 0.0770, Accuracy 0.9706, Time 3.4s\n",
            "Step 400, Loss 0.0624, Accuracy 0.9783, Time 4.9s\n",
            "Step 500, Loss 0.0516, Accuracy 0.9869, Time 5.9s\n",
            "Step 600, Loss 0.1094, Accuracy 0.9714, Time 7.0s\n",
            "Step 700, Loss 0.0699, Accuracy 0.9742, Time 7.9s\n",
            "Step 800, Loss 0.0663, Accuracy 0.9845, Time 9.1s\n",
            "Step 900, Loss 0.0915, Accuracy 0.9742, Time 10.5s\n",
            "Step 1000, Loss 0.0459, Accuracy 0.9879, Time 12.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Reset environment\n",
        "env = dataset.env\n",
        "env.reset(no_step=True)\n",
        "\n",
        "# Initialize variables for logging\n",
        "perf = 0\n",
        "activity_dict = {}  # recording activity\n",
        "trial_infos = {}  # recording trial information\n",
        "\n",
        "num_trial = 200\n",
        "for i in range(num_trial):\n",
        "    # Neurogym boiler plate\n",
        "    # Sample a new trial\n",
        "    trial_info = env.new_trial()\n",
        "    # Observation and groud-truth of this trial\n",
        "    ob, gt = env.ob, env.gt\n",
        "    # Convert to numpy, add batch dimension to input\n",
        "    inputs = torch.from_numpy(ob[:, np.newaxis, :]).type(torch.float)\n",
        "\n",
        "    # Run the network for one trial\n",
        "    # inputs (SeqLen, Batch, InputSize)\n",
        "    # action_pred (SeqLen, Batch, OutputSize)\n",
        "    action_pred, rnn_activity = net(inputs)\n",
        "\n",
        "    # Compute performance\n",
        "    # First convert back to numpy\n",
        "    action_pred = action_pred.detach().numpy()[:, 0, :]\n",
        "    # Read out final choice at last time step\n",
        "    choice = np.argmax(action_pred[-1, :])\n",
        "    # Compare to ground truth\n",
        "    correct = choice == gt[-1]\n",
        "\n",
        "    # Record activity, trial information, choice, correctness\n",
        "    rnn_activity = rnn_activity[:, 0, :].detach().numpy()\n",
        "    activity_dict[i] = rnn_activity\n",
        "    trial_infos[i] = trial_info  # trial_info is a dictionary\n",
        "    trial_infos[i].update({'correct': correct})\n",
        "# Print information for sample trials\n",
        "for i in range(10):\n",
        "    print('Trial ', i, trial_infos[i])\n",
        "\n",
        "print('Average performance', np.mean([val['correct'] for val in trial_infos.values()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWqjVx08i1Tj",
        "outputId": "f6e3af45-77c8-4e68-a153-25d11281ee05"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial  0 {'measure': 1000.0, 'gain': 1, 'production': 1000.0, 'correct': True}\n",
            "Trial  1 {'measure': 1000.0, 'gain': 1, 'production': 1000.0, 'correct': True}\n",
            "Trial  2 {'measure': 800.0, 'gain': 1, 'production': 800.0, 'correct': True}\n",
            "Trial  3 {'measure': 1300.0, 'gain': 1, 'production': 1300.0, 'correct': True}\n",
            "Trial  4 {'measure': 900.0, 'gain': 1, 'production': 900.0, 'correct': True}\n",
            "Trial  5 {'measure': 1200.0, 'gain': 1, 'production': 1200.0, 'correct': True}\n",
            "Trial  6 {'measure': 800.0, 'gain': 1, 'production': 800.0, 'correct': True}\n",
            "Trial  7 {'measure': 900.0, 'gain': 1, 'production': 900.0, 'correct': True}\n",
            "Trial  8 {'measure': 1100.0, 'gain': 1, 'production': 1100.0, 'correct': True}\n",
            "Trial  9 {'measure': 1400.0, 'gain': 1, 'production': 1400.0, 'correct': True}\n",
            "Average performance 1.0\n"
          ]
        }
      ]
    }
  ]
}