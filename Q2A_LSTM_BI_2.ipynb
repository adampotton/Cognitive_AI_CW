{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJEudXFeYgbkWpXQH4rfL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/Cognitive_AI_CW/blob/main/Q2A_LSTM_BI_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adpating the LSTM model architecture to tackle sparsity"
      ],
      "metadata": {
        "id": "sXVSJrSC5bzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/neurogym/neurogym.git\n",
        "%cd neurogym/\n",
        "! pip install -e ."
      ],
      "metadata": {
        "collapsed": true,
        "id": "3gn4uXo2PPtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd82b0fe-b675-41a7-85bd-fcfa33939f03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neurogym'...\n",
            "remote: Enumerating objects: 11100, done.\u001b[K\n",
            "remote: Counting objects: 100% (1002/1002), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 11100 (delta 928), reused 896 (delta 896), pack-reused 10098 (from 1)\u001b[K\n",
            "Receiving objects: 100% (11100/11100), 8.17 MiB | 9.93 MiB/s, done.\n",
            "Resolving deltas: 100% (8333/8333), done.\n",
            "/content/neurogym\n",
            "Obtaining file:///content/neurogym\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (1.26.4)\n",
            "Collecting gym<0.25,>=0.20.0 (from neurogym==0.0.2)\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (3.8.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.25,>=0.20.0->neurogym==0.0.2) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurogym==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurogym==0.0.2) (1.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793123 sha256=a0fee6e86741ae791764548e18d5949841d91499cacf7bba3615ca1dc30d8ae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/fb/19/388995b88cb551717a8dff40c889172cd12fadf994216a0a22\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, neurogym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py develop for neurogym\n",
            "Successfully installed gym-0.24.1 neurogym-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RPIsY6LowX",
        "outputId": "29191ca0-7b29-40b3-ec0b-448919b787dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:396: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "import neurogym as ngym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a simple LSTM model"
      ],
      "metadata": {
        "id": "5xqGhnoAk1Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sparse_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, sparsity_masks):\n",
        "        super(Sparse_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers)  # Standard LSTM with n layers\n",
        "        self.sparsity_masks = sparsity_masks  # Binary masks for sparsity\n",
        "\n",
        "    def forward(self, x): # Applying sparsity on each forward pass\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.lstm.named_parameters():\n",
        "                if 'weight' in name:  # Only applying sparsity mask to weight matrices\n",
        "                    layer_idx = int(name.split('_l')[1].split('.')[0])  # Extract layer index from name\n",
        "                    if 'ih' in name:  # Input-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_ih_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "                    elif 'hh' in name:  # Hidden-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_hh_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        return output, (hn, cn)\n",
        "\n",
        "    def apply_sparsity_masks_after_optimiser(self):  # Applying sparsity masks after the optimiser step\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.lstm.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    layer_idx = int(name.split('_l')[1].split('.')[0])  # Extract layer index from name\n",
        "                    if 'ih' in name:  # Input-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_ih_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "                    elif 'hh' in name:  # Hidden-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_hh_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "\n",
        "\n",
        "class LSTMNet(nn.Module): # Define the main model with the added linear layer\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_lstm_layers=2, sparsity_masks=None):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm = Sparse_LSTM(input_size, hidden_size, num_layers=num_lstm_layers, sparsity_masks=sparsity_masks)  # Sparse LSTM layers\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_output)\n",
        "        return out, lstm_output\n"
      ],
      "metadata": {
        "id": "TvUHo7Q4O0mZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2038de-cff0-4b5f-fc0f-9c346e5c7e96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataset and adjusting parameters"
      ],
      "metadata": {
        "id": "1xE2ARNnlcXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dt': 200, # Timestep parameter\n",
        "    'hidden_size': 32, # Hidden size for your LSTM\n",
        "    'batch_size': 32, # Batch size for training\n",
        "    'seq_len': 50, # Sequence length for input data\n",
        "    'envid': 'ReadySetGo-v0', # Task name\n",
        "    'gain': 2, # Custom gain\n",
        "    'prod_margin': 10, # Custom production margin\n",
        "}\n",
        "\n",
        "env_kwargs = {\n",
        "    'dt': config['dt'], # Assing timestep parameter\n",
        "    'gain': config['gain'],  # Controls the measure that the agent has to produce\n",
        "    'prod_margin': config['prod_margin'], # Controls the interval around the ground truth production time within which the agent receives proportional reward\n",
        "}\n",
        "config['env_kwargs'] = env_kwargs\n",
        "\n",
        "dataset = ngym.Dataset(config['envid'], env_kwargs=config['env_kwargs'], batch_size=config['batch_size'], seq_len=config['seq_len']) # Generate dataset\n",
        "env = dataset.env\n",
        "\n",
        "inputs, target = dataset() # Assing inputs and targets\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "input_size = env.observation_space.shape[0] # Find dimensions for data\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
        "print('Target has shape (SeqLen, Batch) =', target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEqrXBuGlbO6",
        "outputId": "ced0a123-ecf3-4021-8158-8160177d6914"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has shape (SeqLen, Batch, Dim) = torch.Size([50, 32, 3])\n",
            "Target has shape (SeqLen, Batch) = (50, 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate sparisty matricies"
      ],
      "metadata": {
        "id": "JPKJRAFKWiTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sparsity_masks(input_size, hidden_size, num_layers, sparsity):\n",
        "    sparsity_masks = {}  # Dictionary to store masks for each layer\n",
        "    for layer in range(num_layers): # Loop through each layer in the LSTM (2 times num_layers)\n",
        "        ih_shape = (4 * hidden_size, input_size if layer == 0 else hidden_size)  # Input to hidden mask shape\n",
        "        hh_shape = (4 * hidden_size, hidden_size)  # Hidden to hidden mask shape\n",
        "        sparsity_masks[f'weight_ih_l{layer}'] = (torch.rand(ih_shape) > sparsity).float() # Create and name I-to-H mask\n",
        "        sparsity_masks[f'weight_hh_l{layer}'] = (torch.rand(hh_shape) > sparsity).float() # Create and name H-to-H mask\n",
        "    return sparsity_masks"
      ],
      "metadata": {
        "id": "39Hc-41jWmeF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "77PBTEEOlxQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_steps = 3000 # Training loops\n",
        "report_freq = 100 # How often a report on is returned\n",
        "num_lstm_layers = 2 # Number of LSTM layers\n",
        "sparsity = 0.5 # Proportion of 0s in binary masks\n",
        "\n",
        "binary_masks = generate_sparsity_masks(input_size, config['hidden_size'], num_lstm_layers, sparsity)\n",
        "\n",
        "net = LSTMNet(input_size, config['hidden_size'], output_size, num_lstm_layers , binary_masks) # Create an instance of the sparse LSTM\n",
        "\n",
        "def train_model(net, dataset, iter_steps, report_freq):\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.0003) # AdamW optimiser\n",
        "    criterion = nn.CrossEntropyLoss() # Loss funciton\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    start_time = time.time() # Start training timer\n",
        "\n",
        "    for i in range(iter_steps): # Loop over training batches\n",
        "        inputs, labels = dataset() # Generate a set of data\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        optimizer.zero_grad() # Reset gradients\n",
        "        output, _ = net(inputs)\n",
        "        output = output.view(-1, output_size)\n",
        "\n",
        "        loss = criterion(output, labels) # Loss function\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update\n",
        "\n",
        "        net.lstm.apply_sparsity_masks_after_optimiser() # Apply sparsity masks after the optimiser step\n",
        "        batch_acc = (torch.argmax(output, dim=1) == labels).sum().item() / labels.shape[0] # Current batch accuracy\n",
        "        running_loss += loss.item()\n",
        "        running_acc += batch_acc\n",
        "\n",
        "        if i % report_freq == report_freq - 1:\n",
        "            running_loss /= report_freq\n",
        "            running_acc /= report_freq  # average accuracy over the last 100 batches\n",
        "            print('Step {}, Loss {:0.4f}, Accuracy {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, running_acc, time.time() - start_time))\n",
        "            running_loss = 0 # Reset metrics for next report\n",
        "            running_acc = 0\n",
        "    return net\n",
        "\n",
        "\n",
        "net = train_model(net, dataset, iter_steps, report_freq) # Call the training function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "milzVKfVSmQT",
        "outputId": "ed9840ba-bdee-498c-debc-e6a803247a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100, Loss 0.5203, Accuracy 0.9622, Time 2.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to check the sparsity of each layer"
      ],
      "metadata": {
        "id": "1xDL2xXNwy3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sparsity_of_weights(net):\n",
        "    with torch.no_grad():  # Ensure no gradients are computed during the check\n",
        "        for name, param in net.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                zeros = torch.sum(param == 0).item()\n",
        "                total = param.numel()\n",
        "                sparsity = zeros / total\n",
        "                print(f\"Sparsity of {name}: {zeros} zeros out of {total} total weights. Sparsity: {sparsity:.2f}\")\n",
        "\n",
        "check_sparsity_of_weights(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8xBrpUgd898",
        "outputId": "9a670c56-3ebe-4869-a399-d043c92f70ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity of lstm.weight_ih_l0: 85 zeros out of 384 total weights. Sparsity: 0.22\n",
            "Sparsity of lstm.weight_hh_l0: 835 zeros out of 4096 total weights. Sparsity: 0.20\n",
            "Sparsity of lstm.weight_ih_l1: 816 zeros out of 4096 total weights. Sparsity: 0.20\n",
            "Sparsity of lstm.weight_hh_l1: 872 zeros out of 4096 total weights. Sparsity: 0.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = dataset.env # Reset environment\n",
        "env.reset(no_step=True)\n",
        "\n",
        "for i in range(500):\n",
        "    test_inputs, test_labels = dataset()\n",
        "    test_inputs = torch.from_numpy(test_inputs).type(torch.float)\n",
        "    test_labels = torch.from_numpy(test_labels.flatten()).type(torch.long)\n",
        "    test_accs = []\n",
        "    with torch.no_grad():\n",
        "        test_output, _ = net(test_inputs)\n",
        "        test_output = test_output.view(-1, output_size)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        test_loss = criterion(test_output, test_labels)\n",
        "        test_acc = (torch.argmax(test_output, dim=1) == test_labels).sum().item() / test_labels.shape[0]\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "print('Test Accuracy:', np.mean(test_accs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWqjVx08i1Tj",
        "outputId": "577a2c15-eeb1-41f5-c648-6f2cc7680977"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9875\n"
          ]
        }
      ]
    }
  ]
}