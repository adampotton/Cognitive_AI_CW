{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOgZGly0WeYas3Pqs4wA21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/Cognitive_AI_CW/blob/main/Q2A_LSTM_BI_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adpating the LSTM model architecture to tackle sparsity"
      ],
      "metadata": {
        "id": "sXVSJrSC5bzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/neurogym/neurogym.git\n",
        "%cd neurogym/\n",
        "! pip install -e ."
      ],
      "metadata": {
        "collapsed": true,
        "id": "3gn4uXo2PPtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RPIsY6LowX",
        "outputId": "255ba6cc-965a-4f95-b6f1-bba356fb166e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:396: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "import neurogym as ngym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a simple LSTM model"
      ],
      "metadata": {
        "id": "5xqGhnoAk1Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sparse_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, sparsity_masks):\n",
        "        super(Sparse_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers)  # Standard LSTM with n layers\n",
        "        self.sparsity_masks = sparsity_masks  # Binary masks for sparsity\n",
        "\n",
        "    def forward(self, x): # Applying sparsity on each forward pass\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.lstm.named_parameters():\n",
        "                if 'weight' in name:  # Only applying sparsity mask to weight matrices\n",
        "                    layer_idx = int(name.split('_l')[1].split('.')[0])  # Extract layer index from name\n",
        "                    if 'ih' in name:  # Input-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_ih_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "                    elif 'hh' in name:  # Hidden-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_hh_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        return output, (hn, cn)\n",
        "\n",
        "    def apply_sparsity_masks_after_optimiser(self):  # Applying sparsity masks after the optimiser step\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.lstm.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    layer_idx = int(name.split('_l')[1].split('.')[0])  # Extract layer index from name\n",
        "                    if 'ih' in name:  # Input-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_ih_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "                    elif 'hh' in name:  # Hidden-to-hidden weights\n",
        "                        mask = self.sparsity_masks.get(f'weight_hh_l{layer_idx}')\n",
        "                        param.data *= mask\n",
        "\n",
        "\n",
        "class LSTMNet(nn.Module): # Define the main model with the added linear layer\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_lstm_layers=2, sparsity_masks=None):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm = Sparse_LSTM(input_size, hidden_size, num_layers=num_lstm_layers, sparsity_masks=sparsity_masks)  # Sparse LSTM layers\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_output)\n",
        "        return out, lstm_output\n"
      ],
      "metadata": {
        "id": "TvUHo7Q4O0mZ"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataset and adjusting parameters"
      ],
      "metadata": {
        "id": "1xE2ARNnlcXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dt': 200, # Timestep parameter\n",
        "    'hidden_size': 32, # Hidden size for your LSTM\n",
        "    'batch_size': 16, # Batch size for training\n",
        "    'seq_len': 100, # Sequence length for input data\n",
        "    'envid': 'ReadySetGo-v0', # Task name\n",
        "    'gain': 2, # Custom gain\n",
        "    'prod_margin': 10, # Custom production margin\n",
        "}\n",
        "\n",
        "env_kwargs = {\n",
        "    'dt': config['dt'], # Assing timestep parameter\n",
        "    'gain': config['gain'],  # Controls the measure that the agent has to produce\n",
        "    'prod_margin': config['prod_margin'], # Controls the interval around the ground truth production time within which the agent receives proportional reward\n",
        "}\n",
        "config['env_kwargs'] = env_kwargs\n",
        "\n",
        "dataset = ngym.Dataset(config['envid'], env_kwargs=config['env_kwargs'], batch_size=config['batch_size'], seq_len=config['seq_len']) # Generate dataset\n",
        "env = dataset.env\n",
        "\n",
        "inputs, target = dataset() # Assing inputs and targets\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "input_size = env.observation_space.shape[0] # Find dimensions for data\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
        "print('Target has shape (SeqLen, Batch) =', target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEqrXBuGlbO6",
        "outputId": "d96dd4ff-e540-408e-f70c-f4015e388848"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has shape (SeqLen, Batch, Dim) = torch.Size([100, 16, 3])\n",
            "Target has shape (SeqLen, Batch) = (100, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate sparisty matricies"
      ],
      "metadata": {
        "id": "JPKJRAFKWiTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sparsity_masks(input_size, hidden_size, num_layers, sparsity):\n",
        "    sparsity_masks = {}  # Dictionary to store masks for each layer\n",
        "    for layer in range(num_layers): # Loop through each layer in the LSTM (2 times num_layers)\n",
        "        ih_shape = (4 * hidden_size, input_size if layer == 0 else hidden_size)  # Input to hidden mask shape\n",
        "        hh_shape = (4 * hidden_size, hidden_size)  # Hidden to hidden mask shape\n",
        "        sparsity_masks[f'weight_ih_l{layer}'] = (torch.rand(ih_shape) > sparsity).float() # Create and name I-to-H mask\n",
        "        sparsity_masks[f'weight_hh_l{layer}'] = (torch.rand(hh_shape) > sparsity).float() # Create and name H-to-H mask\n",
        "    return sparsity_masks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Hc-41jWmeF",
        "outputId": "f9074576-be24-4f7d-fb70-a75fc56106f1"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "77PBTEEOlxQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_steps = 1000 # Training loops\n",
        "report_freq = 100 # How often a report on is returned\n",
        "num_lstm_layers = 2 # Number of LSTM layers\n",
        "sparsity = 0.2 # Proportion of 0s in binary masks\n",
        "\n",
        "binary_masks = generate_sparsity_masks(input_size, config['hidden_size'], num_lstm_layers, sparsity)\n",
        "\n",
        "net = LSTMNet(input_size, config['hidden_size'], output_size, num_lstm_layers , binary_masks) # Create an instance of the sparse LSTM\n",
        "\n",
        "def train_model(net, dataset, iter_steps, report_freq):\n",
        "\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=0.01) # AdamW optimiser\n",
        "    criterion = nn.CrossEntropyLoss() # Loss funciton\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    start_time = time.time() # Start training timer\n",
        "\n",
        "    for i in range(iter_steps): # Loop over training batches\n",
        "        inputs, labels = dataset() # Generate a set of data\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        optimizer.zero_grad() # Reset gradients\n",
        "        output, _ = net(inputs)\n",
        "        output = output.view(-1, output_size)\n",
        "\n",
        "        loss = criterion(output, labels) # Loss function\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update\n",
        "\n",
        "        net.lstm.apply_sparsity_masks_after_optimiser() # Apply sparsity masks after the optimiser step\n",
        "        batch_acc = (torch.argmax(output, dim=1) == labels).sum().item() / labels.shape[0] # Current batch accuracy\n",
        "        running_loss += loss.item()\n",
        "        running_acc += batch_acc\n",
        "\n",
        "        if i % report_freq == report_freq - 1:\n",
        "            running_loss /= report_freq\n",
        "            running_acc /= report_freq  # average accuracy over the last 100 batches\n",
        "            print('Step {}, Loss {:0.4f}, Accuracy {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, running_acc, time.time() - start_time))\n",
        "            running_loss = 0 # Reset metrics for next report\n",
        "            running_acc = 0\n",
        "    return net\n",
        "\n",
        "\n",
        "net = train_model(net, dataset, iter_steps, report_freq) # Call the training function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "milzVKfVSmQT",
        "outputId": "4d898cf5-c66c-4c76-8dbb-a0650c2b4e68"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100, Loss 0.1802, Accuracy 0.9622, Time 1.9s\n",
            "Step 200, Loss 0.1264, Accuracy 0.9622, Time 3.9s\n",
            "Step 300, Loss 0.0735, Accuracy 0.9697, Time 6.4s\n",
            "Step 400, Loss 0.0228, Accuracy 0.9949, Time 8.7s\n",
            "Step 500, Loss 0.0183, Accuracy 0.9950, Time 11.6s\n",
            "Step 600, Loss 0.0158, Accuracy 0.9958, Time 13.5s\n",
            "Step 700, Loss 0.0158, Accuracy 0.9957, Time 15.6s\n",
            "Step 800, Loss 0.0156, Accuracy 0.9958, Time 17.7s\n",
            "Step 900, Loss 0.0150, Accuracy 0.9958, Time 19.7s\n",
            "Step 1000, Loss 0.0157, Accuracy 0.9956, Time 22.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to check the sparsity of each layer"
      ],
      "metadata": {
        "id": "1xDL2xXNwy3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sparsity_of_weights(net):\n",
        "    with torch.no_grad():  # Ensure no gradients are computed during the check\n",
        "        for name, param in net.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                zeros = torch.sum(param == 0).item()\n",
        "                total = param.numel()\n",
        "                sparsity = zeros / total\n",
        "                print(f\"Sparsity of {name}: {zeros} zeros out of {total} total weights. Sparsity: {sparsity:.2f}\")\n",
        "\n",
        "check_sparsity_of_weights(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8xBrpUgd898",
        "outputId": "e83ac663-3e82-4965-f7c9-5b512397c823"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity of lstm.weight_ih_l0: 99 zeros out of 384 total weights. Sparsity: 0.26\n",
            "Sparsity of lstm.weight_hh_l0: 827 zeros out of 4096 total weights. Sparsity: 0.20\n",
            "Sparsity of lstm.weight_ih_l1: 824 zeros out of 4096 total weights. Sparsity: 0.20\n",
            "Sparsity of lstm.weight_hh_l1: 831 zeros out of 4096 total weights. Sparsity: 0.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = dataset.env # Reset environment\n",
        "env.reset(no_step=True)\n",
        "\n",
        "perf = 0 # Initialize loggin vars\n",
        "activity_dict = {}\n",
        "trial_infos = {}\n",
        "\n",
        "num_trial = 200\n",
        "for i in range(num_trial):\n",
        "\n",
        "    trial_info = env.new_trial() # New trial\n",
        "    ob, gt = env.ob, env.gt # Observation and groud-truth of this trial\n",
        "    inputs = torch.from_numpy(ob[:, np.newaxis, :]).type(torch.float)\n",
        "\n",
        "    action_pred, rnn_activity = net(inputs) # Run network for one trial\n",
        "\n",
        "    action_pred = action_pred.detach().numpy()[:, 0, :] # Compute performance\n",
        "    choice = np.argmax(action_pred[-1, :]) # Final choice at final time step\n",
        "    correct = choice == gt[-1]\n",
        "\n",
        "    rnn_activity = rnn_activity[:, 0, :].detach().numpy() # Record activity\n",
        "    activity_dict[i] = rnn_activity\n",
        "    trial_infos[i] = trial_info  # Record trial infos\n",
        "    trial_infos[i].update({'correct': correct})\n",
        "\n",
        "for i in range(20):\n",
        "    print('Trial ', i, trial_infos[i])\n",
        "\n",
        "print('Average performance', np.mean([val['correct'] for val in trial_infos.values()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWqjVx08i1Tj",
        "outputId": "0ce66eb4-65eb-4c2c-9bbc-29c3fde2ee2e"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial  0 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  1 {'measure': 1400.0, 'gain': 2, 'production': 2800.0, 'correct': True}\n",
            "Trial  2 {'measure': 1400.0, 'gain': 2, 'production': 2800.0, 'correct': True}\n",
            "Trial  3 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  4 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  5 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  6 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  7 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  8 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  9 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  10 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  11 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  12 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  13 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  14 {'measure': 1400.0, 'gain': 2, 'production': 2800.0, 'correct': True}\n",
            "Trial  15 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  16 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  17 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  18 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  19 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Average performance 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}