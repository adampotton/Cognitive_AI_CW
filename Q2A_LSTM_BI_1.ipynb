{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCwDBWGC5WPW9flShtn8FX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adampotton/Cognitive_AI_CW/blob/main/Q2A_LSTM_BI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adpating the LSTM model cost function to tackle sparsity"
      ],
      "metadata": {
        "id": "sXVSJrSC5bzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/neurogym/neurogym.git\n",
        "%cd neurogym/\n",
        "! pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3gn4uXo2PPtk",
        "outputId": "11979d81-0947-49fc-c873-514fab115fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neurogym'...\n",
            "remote: Enumerating objects: 11100, done.\u001b[K\n",
            "remote: Counting objects: 100% (1001/1001), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 11100 (delta 928), reused 898 (delta 898), pack-reused 10099 (from 1)\u001b[K\n",
            "Receiving objects: 100% (11100/11100), 8.17 MiB | 10.97 MiB/s, done.\n",
            "Resolving deltas: 100% (8335/8335), done.\n",
            "/content/neurogym\n",
            "Obtaining file:///content/neurogym\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurogym==0.0.2) (1.26.4)\n",
            "Collecting gym<0.25,>=0.20.0 (from neurogym==0.0.2)\n",
            "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RPIsY6LowX",
        "outputId": "c103a53d-ed6e-4395-9994-68201045d0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:396: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "import neurogym as ngym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a simple LSTM model"
      ],
      "metadata": {
        "id": "5xqGhnoAk1Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_lstm_layers = 2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_lstm_layers) # n LSTM layers\n",
        "        self.fc = nn.Linear(hidden_size, output_size) # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_output)\n",
        "        return out, lstm_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUHo7Q4O0mZ",
        "outputId": "b78d4a43-27ac-471a-b5e7-f9f8e23e0c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataset and adjusting parameters"
      ],
      "metadata": {
        "id": "1xE2ARNnlcXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dt': 200, # Timestep parameter\n",
        "    'hidden_size': 32, # Hidden size for your LSTM\n",
        "    'batch_size': 32, # Batch size for training\n",
        "    'seq_len': 50, # Sequence length for input data\n",
        "    'envid': 'ReadySetGo-v0', # Task name\n",
        "    'gain': 2, # Custom gain\n",
        "    'prod_margin': 10, # Custom production margin\n",
        "}\n",
        "\n",
        "env_kwargs = {\n",
        "    'dt': config['dt'], # Assing timestep parameter\n",
        "    'gain': config['gain'],  # Controls the measure that the agent has to produce\n",
        "    'prod_margin': config['prod_margin'], # Controls the interval around the ground truth production time within which the agent receives proportional reward\n",
        "}\n",
        "config['env_kwargs'] = env_kwargs\n",
        "\n",
        "dataset = ngym.Dataset(config['envid'], env_kwargs=config['env_kwargs'], batch_size=config['batch_size'], seq_len=config['seq_len']) # Generate dataset\n",
        "env = dataset.env\n",
        "\n",
        "inputs, target = dataset() # Assing inputs and targets\n",
        "inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "\n",
        "input_size = env.observation_space.shape[0] # Find dimensions for data\n",
        "output_size = env.action_space.n\n",
        "\n",
        "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
        "print('Target has shape (SeqLen, Batch) =', target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEqrXBuGlbO6",
        "outputId": "df3f686e-7cb4-47c3-bbb9-b10657a14a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has shape (SeqLen, Batch, Dim) = torch.Size([100, 16, 3])\n",
            "Target has shape (SeqLen, Batch) = (100, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:69: UserWarning: \u001b[33mWARN: Agent's minimum action space value is -infinity. This is probably too low.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:73: UserWarning: \u001b[33mWARN: Agent's maximum action space value is infinity. This is probably too high\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "77PBTEEOlxQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_steps = 3000 # Training loops\n",
        "report_freq = 50 # How often a report on is returned\n",
        "l1_beta = 0.75 # Sparsity control\n",
        "\n",
        "net = LSTMNet(input_size, config['hidden_size'], output_size, num_lstm_layers = 1) # Create an instance of the LSTM\n",
        "\n",
        "def train_model(net, dataset, iter_steps, report_freq, l1_beta):\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.0003) # Adam optimiser\n",
        "    criterion = nn.CrossEntropyLoss() # Loss funciton\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    start_time = time.time() # Start training timer\n",
        "\n",
        "    for i in range(iter_steps): # Loop over training batches\n",
        "        inputs, labels = dataset() # Generate a set of data\n",
        "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
        "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
        "\n",
        "        optimizer.zero_grad() # Reset gradients\n",
        "        output, _ = net(inputs)\n",
        "        output = output.view(-1, output_size)\n",
        "\n",
        "\n",
        "        l1_reg = 0 # Initialise the L1 Regularisation term\n",
        "        for param in net.parameters(): # Extract all weight matrices from the network\n",
        "            if param.requires_grad: # ensure that regularisation is only applied to trainable parts of the model\n",
        "                l1_reg += torch.sum(torch.abs(param)) # Sum the matrices\n",
        "        loss = criterion(output, labels) + l1_beta * l1_reg # Define new modified loss funciton\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()  # Update\n",
        "\n",
        "        batch_acc = (torch.argmax(output, dim=1) == labels).sum().item() / labels.shape[0] # Current batch accuracy\n",
        "        running_loss += loss.item()\n",
        "        running_acc += batch_acc\n",
        "\n",
        "        if i % report_freq == report_freq - 1:\n",
        "            running_loss /= report_freq\n",
        "            running_acc /= report_freq  # average accuracy over the last 100 batches\n",
        "            print('Step {}, Loss {:0.4f}, Accuracy {:0.4f}, Time {:0.1f}s'.format(\n",
        "                i+1, running_loss, running_acc, time.time() - start_time))\n",
        "            running_loss = 0 # Reset metrics for next report\n",
        "            running_acc = 0\n",
        "    return net\n",
        "\n",
        "net = train_model(net, dataset, iter_steps, report_freq, l1_beta) # Call the training function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "milzVKfVSmQT",
        "outputId": "82aaf52d-1f06-4e96-f8bf-be9dbb6355c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100, Loss 32.1535, Accuracy 0.9281, Time 1.3s\n",
            "Step 200, Loss 5.2581, Accuracy 0.9621, Time 2.5s\n",
            "Step 300, Loss 5.2538, Accuracy 0.9254, Time 4.3s\n",
            "Step 400, Loss 5.2488, Accuracy 0.9436, Time 6.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measuring the sparsity"
      ],
      "metadata": {
        "id": "3fNAW3kCWX3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparsity = 0\n",
        "total_params = 0\n",
        "for param in net.parameters():\n",
        "    if param.requires_grad:\n",
        "        total_params += param.numel()\n",
        "        sparsity += torch.sum(param <= 0.001).item()\n",
        "\n",
        "sparsity_ratio = sparsity / total_params\n",
        "print('Sparsity ratio:', sparsity_ratio)"
      ],
      "metadata": {
        "id": "T2lTaCuB6gfK",
        "outputId": "c3ce38e3-a371-47b4-b89d-10c9b5ca05a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity ratio: 0.5041649312786339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Peformance Metrics"
      ],
      "metadata": {
        "id": "a8eAE01F2ESX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_losses = []\n",
        "avg_accuracies = []\n",
        "report_freq = 20\n",
        "for i in range(report_freq - 1, len(losses), report_freq):\n",
        "    avg_losses.append(sum(losses[i-report_freq+1:i+1]) / report_freq)\n",
        "    avg_accuracies.append(sum(accuracies[i-report_freq+1:i+1]) / report_freq)\n",
        "\n",
        "iterations = range(report_freq, len(losses) + 1, report_freq)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(iterations, avg_losses, label='Loss', color='blue')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(iterations, avg_accuracies, label='Accuracy', color='green')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.8, 1.05)\n",
        "plt.title('Training Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Np1xonD42DXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = dataset.env # Reset environment\n",
        "env.reset(no_step=True)\n",
        "\n",
        "perf = 0 # Initialize loggin vars\n",
        "activity_dict = {}\n",
        "trial_infos = {}\n",
        "\n",
        "num_trial = 200\n",
        "for i in range(num_trial):\n",
        "\n",
        "    trial_info = env.new_trial() # New trial\n",
        "    ob, gt = env.ob, env.gt # Observation and groud-truth of this trial\n",
        "    inputs = torch.from_numpy(ob[:, np.newaxis, :]).type(torch.float)\n",
        "\n",
        "    action_pred, rnn_activity = net(inputs) # Run network for one trial\n",
        "\n",
        "    action_pred = action_pred.detach().numpy()[:, 0, :] # Compute performance\n",
        "    choice = np.argmax(action_pred[-1, :]) # Final choice at final time step\n",
        "    correct = choice == gt[-1]\n",
        "\n",
        "    rnn_activity = rnn_activity[:, 0, :].detach().numpy() # Record activity\n",
        "    activity_dict[i] = rnn_activity\n",
        "    trial_infos[i] = trial_info  # Record trial infos\n",
        "    trial_infos[i].update({'correct': correct})\n",
        "\n",
        "for i in range(20):\n",
        "    print('Trial ', i, trial_infos[i])\n",
        "\n",
        "print('Average performance', np.mean([val['correct'] for val in trial_infos.values()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWqjVx08i1Tj",
        "outputId": "474032fd-4c56-4e54-957b-561cc6d989c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial  0 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  1 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  2 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  3 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  4 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  5 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  6 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  7 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  8 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  9 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  10 {'measure': 1400.0, 'gain': 2, 'production': 2800.0, 'correct': True}\n",
            "Trial  11 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  12 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  13 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  14 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  15 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Trial  16 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  17 {'measure': 800.0, 'gain': 2, 'production': 1600.0, 'correct': True}\n",
            "Trial  18 {'measure': 1000.0, 'gain': 2, 'production': 2000.0, 'correct': True}\n",
            "Trial  19 {'measure': 1200.0, 'gain': 2, 'production': 2400.0, 'correct': True}\n",
            "Average performance 1.0\n"
          ]
        }
      ]
    }
  ]
}